#!/usr/bin/env python3
"""
Blog Writer Demo using Felix Framework Geometric Orchestration.

This demo showcases how the Felix Framework's helix-based multi-agent system
can be used for collaborative content creation, demonstrating the geometric
orchestration approach as an alternative to traditional multi-agent systems.

The demo creates a team of specialized LLM agents that work together to write
a blog post, with agents spawning at different times and converging naturally
through the helix geometry toward a final synthesis.

Usage:
    python examples/blog_writer.py "Write a blog post about quantum computing"

Requirements:
    - LM Studio running with a model loaded (http://localhost:1234)
    - openai Python package installed
"""

import sys
import time
import asyncio
import argparse
from typing import List, Dict, Any
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from core.helix_geometry import HelixGeometry
from llm.lm_studio_client import LMStudioClient, LMStudioConnectionError
from llm.token_budget import TokenBudgetManager
from agents.llm_agent import LLMTask
from agents.specialized_agents import create_specialized_team
from communication.central_post import CentralPost, AgentFactory
from communication.spoke import SpokeManager


class FelixBlogWriter:
    """
    Blog writing system using Felix geometric orchestration.
    
    Demonstrates how helix-based agent coordination can create
    content through natural convergence rather than explicit
    workflow management.
    """
    
    def __init__(self, lm_studio_url: str = "http://localhost:1234/v1", random_seed: int = None):
        """
        Initialize the Felix blog writing system.
        
        Args:
            lm_studio_url: LM Studio API endpoint
            random_seed: Seed for randomization (None for truly random behavior)
        """
        self.random_seed = random_seed
        
        # Create helix geometry (OpenSCAD parameters)
        self.helix = HelixGeometry(
            top_radius=33.0,
            bottom_radius=0.001,
            height=33.0,
            turns=33
        )
        
        # Initialize LLM client
        self.llm_client = LMStudioClient(base_url=lm_studio_url)
        
        # Initialize communication system
        self.central_post = CentralPost(max_agents=20, enable_metrics=True)
        self.spoke_manager = SpokeManager(self.central_post)
        
        # Initialize token budget manager
        self.token_budget_manager = TokenBudgetManager(
            base_budget=1200,  # Increased budget for blog writing
            min_budget=150,
            max_budget=800
        )
        
        # Initialize agent factory for dynamic agent creation
        self.agent_factory = AgentFactory(
            helix=self.helix,
            llm_client=self.llm_client,
            token_budget_manager=self.token_budget_manager,
            random_seed=random_seed
        )
        
        # Agent team
        self.agents = []
        
        print(f"Felix Blog Writer initialized")
        print(f"Helix: {self.helix.turns} turns, {self.helix.top_radius}â†’{self.helix.bottom_radius} radius")
        if random_seed is not None:
            print(f"Random seed: {random_seed} (deterministic behavior)")
        else:
            print("Random seed: None (truly random behavior)")
    
    def test_lm_studio_connection(self) -> bool:
        """Test connection to LM Studio."""
        try:
            if self.llm_client.test_connection():
                print("âœ“ LM Studio connection successful")
                return True
            else:
                print("âœ— LM Studio connection failed")
                return False
        except LMStudioConnectionError as e:
            print(f"âœ— LM Studio connection error: {e}")
            return False
    
    def create_blog_writing_team(self, complexity: str = "medium") -> None:
        """
        Create specialized team for blog writing with randomized spawn times.
        
        Args:
            complexity: Task complexity level
        """
        print(f"\nCreating {complexity} complexity blog writing team...")
        
        # Create specialized agents with randomized spawn times
        self.agents = create_specialized_team(
            helix=self.helix,
            llm_client=self.llm_client,
            task_complexity=complexity,
            token_budget_manager=self.token_budget_manager,
            random_seed=self.random_seed
        )
        
        # Register agents with communication system
        for agent in self.agents:
            self.spoke_manager.register_agent(agent)
        
        print(f"Created team of {len(self.agents)} specialized agents:")
        for agent in self.agents:
            print(f"  - {agent.agent_id} ({agent.agent_type}) spawns at t={agent.spawn_time:.3f}")
        print(f"Agent spawn times are {'deterministic' if self.random_seed is not None else 'randomized'}")
    
    async def run_blog_writing_session_async(self, topic: str, simulation_time: float = 1.0) -> Dict[str, Any]:
        """
        Run collaborative blog writing session with true parallel processing.
        
        Args:
            topic: Blog post topic
            simulation_time: Duration of simulation (0.0 to 1.0)
            
        Returns:
            Results from the writing session
        """
        print(f"\n{'='*60}")
        print(f"FELIX PARALLEL BLOG WRITING SESSION")
        print(f"Topic: {topic}")
        print(f"Mode: {'STRICT (Lightweight)' if self.strict_mode else 'NORMAL'}")
        print(f"{'='*60}")
        
        # Start async message processing
        await self.central_post.start_async_processing(max_concurrent_processors=2)
        
        # Create the main task
        main_task = LLMTask(
            task_id="blog_post_001",
            description=f"Write a comprehensive blog post about: {topic}",
            context=f"This is a collaborative writing project. Multiple agents will contribute research, analysis, and synthesis to create a high-quality blog post about {topic}."
        )
        
        # Track session results
        results = {
            "topic": topic,
            "agents_participated": [],
            "processing_timeline": [],
            "final_output": None,
            "session_stats": {},
            "parallel_processing": True
        }
        
        session_start = time.perf_counter()
        
        # Performance targets for strict mode
        if self.strict_mode:
            target_completion_time = 30.0  # 30 seconds max
            target_total_tokens = 2000     # 2000 tokens max
        else:
            target_completion_time = simulation_time * 60  # More relaxed
            target_total_tokens = 10000
        
        print(f"\nStarting PARALLEL geometric orchestration...")
        print(f"Target: {'<30s, <2000 tokens' if self.strict_mode else 'Normal performance'}")
        
        # Run parallel processing
        final_result = await self._run_parallel_processing(
            main_task, 0.0, 0.05, simulation_time, results,
            target_completion_time, target_total_tokens
        )
        
        session_end = time.perf_counter()
        session_duration = session_end - session_start
        
        # Cleanup async resources
        await self.central_post.shutdown_async()
        if hasattr(self.llm_client, 'close_async'):
            await self.llm_client.close_async()
        
        # Final statistics
        total_tokens = sum(a.get("tokens_used", 0) for a in results["agents_participated"])
        results["session_stats"] = {
            "total_duration": session_duration,
            "total_tokens_used": total_tokens,
            "agents_created": len(self.agents),
            "agents_participated": len(results["agents_participated"]),
            "total_messages_processed": self.central_post.total_messages_processed,
            "llm_client_stats": self.llm_client.get_usage_stats(),
            "strict_mode": self.strict_mode,
            "performance_targets_met": {
                "time_target": session_duration < target_completion_time,
                "token_target": total_tokens < target_total_tokens
            }
        }
        
        return results
    
    def run_blog_writing_session(self, topic: str, simulation_time: float = 1.0) -> Dict[str, Any]:
        """
        Run collaborative blog writing session.
        
        Args:
            topic: Blog post topic
            simulation_time: Duration of simulation (0.0 to 1.0)
            
        Returns:
            Results from the writing session
        """
        # Run the async version
        return asyncio.run(self.run_blog_writing_session_async(topic, simulation_time))
    
    def display_results(self, results: Dict[str, Any]) -> None:
                    
                    print(f"\n[t={current_time:.2f}] ðŸŒ€ Spawning {agent.agent_id} ({agent.agent_type}) at helix top")
                    agent.spawn(current_time, main_task)
            
            # Process all active agents as they descend the helix
            for agent in self.agents:
                if agent.state.value == "active":
                    # Update agent position (descend the helix)
                    agent.update_position(current_time)
                    
                    # Get current position info
                    pos_info = agent.get_position_info(current_time)
                    depth = pos_info.get("depth_ratio", 0.0)
                    radius = pos_info.get("radius", 0.0)
                    
                    # Continuous processing: agents work as they descend
                    # Process every few steps to show progression
                    if current_time % 0.1 < time_step or agent.processing_stage == 0:
                        try:
                            result = agent.process_task_with_llm(main_task, current_time)
                            
                            print(f"  [{current_time:.2f}] ðŸ”„ {agent.agent_id} processing (stage {result.processing_stage})")
                            print(f"      Depth: {depth:.2f}, Confidence: {result.confidence:.2f}")
                            
                            # Share result with central post
                            message = agent.share_result_to_central(result)
                            self.spoke_manager.send_message(agent.agent_id, message)
                            
                            # Check for high-confidence acceptance
                            if self.central_post.accept_high_confidence_result(message):
                                print(f"      âœ… HIGH CONFIDENCE: {agent.agent_id} result accepted by central post!")
                                print(f"      ðŸŽ¯ SIMULATION COMPLETE: Central post absorbed high-confidence result!")
                                
                                # Set as final output
                                results["final_output"] = {
                                    "content": result.content,
                                    "agent_id": agent.agent_id,
                                    "confidence": result.confidence,
                                    "stage": result.processing_stage,
                                    "timestamp": current_time
                                }
                                
                                simulation_complete = True
                            
                            # Track results only for final output (when confidence is high enough)
                            if result.confidence >= 0.6:  # Lower threshold for tracking
                                results["agents_participated"].append({
                                    "agent_id": agent.agent_id,
                                    "agent_type": agent.agent_type,
                                    "spawn_time": current_time,
                                    "position_info": result.position_info,
                                    "content_preview": result.content[:100] + "...",
                                    "tokens_used": result.llm_response.tokens_used,
                                    "processing_time": result.processing_time,
                                    "confidence": result.confidence,
                                    "stage": result.processing_stage
                                })
                                
                                results["processing_timeline"].append({
                                    "timestamp": current_time,
                                    "agent_id": agent.agent_id,
                                    "action": "task_processed",
                                    "tokens": result.llm_response.tokens_used,
                                    "confidence": result.confidence
                                })
                                
                                # Final output is now set when central post accepts high-confidence result
                        
                        except Exception as e:
                            print(f"  âœ— Error processing task for {agent.agent_id}: {e}")
            
            # Update active agents' positions
            for agent in self.agents:
                if agent.state.value == "active":
                    agent.update_position(current_time)
            
            # Dynamic agent spawning: assess if new agents are needed
            if current_time % 0.2 < time_step:  # Check every 0.2 time units
                recommended_agents = self.agent_factory.assess_team_needs(
                    self.central_post._processed_messages, current_time
                )
                
                if recommended_agents:
                    print(f"\n[t={current_time:.2f}] ðŸš€ Dynamic spawning {len(recommended_agents)} new agents:")
                    for new_agent in recommended_agents:
                        self.agents.append(new_agent)
                        self.spoke_manager.register_agent(new_agent)
                        print(f"    - {new_agent.agent_id} ({new_agent.agent_type}) spawns at t={new_agent.spawn_time:.3f}")
            
            # Emergent behavior: process agent interactions
            if current_time % 0.15 < time_step:  # Check every 0.15 time units
                self._process_agent_interactions(current_time)
            
            # Process communication system and advance time
            self.spoke_manager.process_all_messages()
            current_time += time_step
        
        session_end = time.perf_counter()
        session_duration = session_end - session_start
        
        # Check if simulation completed due to timeout
        if not simulation_complete and current_time >= max_timeout:
            print(f"\nâš ï¸  WARNING: Simulation reached maximum timeout ({max_timeout:.1f} time units)")
            print(f"    No agent achieved sufficient confidence for central post absorption.")
            print(f"    Consider increasing agent processing time or adjusting confidence thresholds.")
        elif simulation_complete:
            print(f"\nâœ… Simulation completed successfully at t={current_time:.2f}")
        
        # Collect final statistics
        results["session_stats"] = {
            "total_duration": session_duration,
            "simulation_time": current_time,  # Actual time simulation ran to
            "max_timeout": max_timeout,       # Maximum allowed time
            "simulation_complete": simulation_complete,
            "agents_created": len(self.agents),
            "agents_participated": len(results["agents_participated"]),
            "total_tokens_used": sum(a["tokens_used"] for a in results["agents_participated"]),
            "total_messages_processed": self.central_post.total_messages_processed,
            "llm_client_stats": self.llm_client.get_usage_stats()
        }
        
        return results
    
    async def _run_parallel_processing(self, main_task: LLMTask, current_time: float, 
                                     time_step: float, simulation_time: float,
                                     results: Dict[str, Any], target_completion_time: float,
                                     target_total_tokens: int) -> Optional[Dict[str, Any]]:
        """
        Run the core parallel processing loop.
        """
        simulation_complete = False
        max_timeout = simulation_time * 10
        
        while current_time <= max_timeout and not simulation_complete:
            step_start = time.perf_counter()
            
            # Collect agents ready for processing
            ready_agents = []
            for agent in self.agents:
                if (agent.can_spawn(current_time) and 
                    agent.state.value == "waiting"):
                    print(f"\n[t={current_time:.2f}] ðŸŒ€ Spawning {agent.agent_id} ({agent.agent_type})")
                    agent.spawn(current_time, main_task)
                    ready_agents.append(agent)
                elif agent.state.value == "active":
                    agent.update_position(current_time)
                    # Process every few steps or first stage
                    if current_time % 0.1 < time_step or agent.processing_stage == 0:
                        ready_agents.append(agent)
            
            # Process agents in parallel batches
            if ready_agents:
                batch_size = min(len(ready_agents), self.max_concurrent_agents)
                for i in range(0, len(ready_agents), batch_size):
                    batch = ready_agents[i:i + batch_size]
                    
                    # Process batch in parallel
                    print(f"[t={current_time:.2f}] ðŸš€ Processing {len(batch)} agents in parallel")
                    
                    batch_results = await self._process_agent_batch_async(
                        batch, main_task, current_time
                    )
                    
                    # Check results and update
                    for result in batch_results:
                        if result:
                            # Check for high-confidence acceptance
                            message = result['agent'].share_result_to_central(result['llm_result'])
                            await self.central_post.queue_message_async(message)
                            
                            if self.central_post.accept_high_confidence_result(message):
                                print(f"      âœ… HIGH CONFIDENCE: {result['agent'].agent_id} result accepted!")
                                print(f"      ðŸŽ¯ PARALLEL SIMULATION COMPLETE!")
                                
                                results["final_output"] = {
                                    "content": result['llm_result'].content,
                                    "agent_id": result['agent'].agent_id,
                                    "confidence": result['llm_result'].confidence,
                                    "stage": result['llm_result'].processing_stage,
                                    "timestamp": current_time
                                }
                                simulation_complete = True
                                break
                            
                            # Track participation
                            if result['llm_result'].confidence >= 0.6:
                                agent_info = {
                                    "agent_id": result['agent'].agent_id,
                                    "agent_type": result['agent'].agent_type,
                                    "spawn_time": current_time,
                                    "position_info": result['llm_result'].position_info,
                                    "tokens_used": result['llm_result'].llm_response.tokens_used,
                                    "confidence": result['llm_result'].confidence,
                                    "stage": result['llm_result'].processing_stage
                                }
                                results["agents_participated"].append(agent_info)
                    
                    if simulation_complete:
                        break
            
            # Check performance targets in strict mode
            if self.strict_mode:
                current_duration = time.perf_counter() - step_start
                current_tokens = sum(a.get("tokens_used", 0) for a in results["agents_participated"])
                
                if current_duration > target_completion_time or current_tokens > target_total_tokens:
                    print(f"\nâš ï¸  Performance target exceeded - completing simulation")
                    simulation_complete = True
                    break
            
            current_time += time_step
        
        return results["final_output"] if simulation_complete else None
    
    async def _process_agent_batch_async(self, agents: List, main_task: LLMTask, 
                                       current_time: float) -> List[Dict[str, Any]]:
        """
        Process a batch of agents in parallel.
        """
        async def process_single_agent(agent):
            try:
                # Determine priority based on agent type and strict mode
                if self.strict_mode:
                    priority = RequestPriority.HIGH  # High priority in strict mode
                else:
                    priority = RequestPriority.NORMAL
                
                result = await agent.process_task_with_llm_async(
                    main_task, current_time, priority
                )
                
                pos_info = agent.get_position_info(current_time)
                depth = pos_info.get("depth_ratio", 0.0)
                
                print(f"    âœ“ {agent.agent_id} completed (depth: {depth:.2f}, "
                      f"confidence: {result.confidence:.2f}, tokens: {result.llm_response.tokens_used})")
                
                return {
                    "agent": agent,
                    "llm_result": result,
                    "success": True
                }
                
            except Exception as e:
                print(f"    âœ— {agent.agent_id} failed: {e}")
                return {
                    "agent": agent,
                    "llm_result": None,
                    "success": False,
                    "error": str(e)
                }
        
        # Process all agents in the batch concurrently
        results = await asyncio.gather(
            *[process_single_agent(agent) for agent in agents],
            return_exceptions=True
        )
        
        # Filter out exceptions and failed results
        successful_results = []
        for result in results:
            if isinstance(result, dict) and result.get("success", False):
                successful_results.append(result)
        
        return successful_results
    
    def display_results(self, results: Dict[str, Any]) -> None:
        """Display session results in a readable format."""
        print(f"\n{'='*60}")
        print(f"SESSION RESULTS")
        print(f"{'='*60}")
        
        stats = results["session_stats"]
        print(f"Topic: {results['topic']}")
        print(f"Duration: {stats['total_duration']:.2f} seconds")
        print(f"Simulation Time: {stats['simulation_time']:.2f} units (completed: {stats['simulation_complete']})")
        print(f"Agents: {stats['agents_participated']}/{stats['agents_created']} participated")
        print(f"Tokens: {stats['total_tokens_used']} total")
        print(f"Messages: {stats['total_messages_processed']} processed")
        
        # Show token budget summary if available
        if hasattr(self, 'token_budget_manager'):
            budget_status = self.token_budget_manager.get_system_status()
            print(f"Token Efficiency: {budget_status['system_efficiency']:.1%} "
                  f"({budget_status['total_used']}/{budget_status['total_allocated']} allocated)")
        
        print(f"\nAgent Participation Timeline:")
        for agent_info in results["agents_participated"]:
            print(f"  {agent_info['spawn_time']:.2f}s: {agent_info['agent_id']} ({agent_info['agent_type']})")
            print(f"        Depth: {agent_info['position_info'].get('depth_ratio', 0):.2f}, "
                  f"Tokens: {agent_info['tokens_used']}")
        
        if results["final_output"]:
            print(f"\n{'='*60}")
            print(f"FINAL BLOG POST")
            print(f"{'='*60}")
            print(results["final_output"]["content"])
            print(f"\n[Generated by {results['final_output']['agent_id']} "
                  f"with confidence {results['final_output']['confidence']:.2f} "
                  f"at stage {results['final_output']['stage']} "
                  f"at time t={results['final_output']['timestamp']:.2f}]")
        else:
            print(f"\nNo final synthesis output was generated.")
    
    def _process_agent_interactions(self, current_time: float) -> None:
        """Process emergent behavior and agent interactions."""
        active_agents = [agent for agent in self.agents if agent.state.value == "active"]
        
        if len(active_agents) < 2:
            return  # Need at least 2 agents for interactions
        
        interactions_processed = 0
        
        for agent in active_agents:
            # Assess collaboration opportunities
            opportunities = agent.assess_collaboration_opportunities(active_agents, current_time)
            
            # Process top collaboration opportunities
            for opportunity in opportunities[:2]:  # Limit to top 2 to avoid chaos
                other_agent_id = opportunity["agent_id"]
                other_agent = next((a for a in active_agents if a.agent_id == other_agent_id), None)
                
                if other_agent and opportunity["compatibility"] > 0.6:
                    influence_type = opportunity["recommended_influence"]
                    influence_strength = opportunity["compatibility"] * 0.5  # Moderate influence
                    
                    agent.influence_agent_behavior(other_agent, influence_type, influence_strength)
                    interactions_processed += 1
                    
                    if interactions_processed % 5 == 0:  # Report occasional interactions
                        print(f"  [t={current_time:.2f}] ðŸ¤ {agent.agent_id} â†’ {other_agent.agent_id}: "
                              f"{influence_type} (strength: {influence_strength:.2f})")
    
    def save_results(self, results: Dict[str, Any], output_file: str = None) -> None:
        """Save results to file."""
        if output_file is None:
            timestamp = int(time.time())
            output_file = f"blog_writing_session_{timestamp}.json"
        
        import json
        with open(output_file, 'w') as f:
            # Make results JSON serializable
            serializable_results = {
                "topic": results["topic"],
                "agents_participated": results["agents_participated"],
                "session_stats": results["session_stats"],
                "final_output": results["final_output"]
            }
            json.dump(serializable_results, f, indent=2)
        
        print(f"\nResults saved to: {output_file}")


def main():
    """Main function for blog writer demo."""
    parser = argparse.ArgumentParser(description="Felix Framework Blog Writer Demo")
    parser.add_argument("topic", help="Blog post topic")
    parser.add_argument("--complexity", choices=["simple", "medium", "complex"], 
                       default="medium", help="Task complexity level")
    parser.add_argument("--simulation-time", type=float, default=1.0,
                       help="Simulation duration (0.0 to 1.0)")
    parser.add_argument("--lm-studio-url", default="http://localhost:1234/v1",
                       help="LM Studio API URL")
    parser.add_argument("--save-output", help="Save results to file")
    parser.add_argument("--random-seed", type=int, 
                       help="Random seed for reproducibility (omit for truly random)")
    
    args = parser.parse_args()
    
    # Create blog writer
    writer = FelixBlogWriter(lm_studio_url=args.lm_studio_url, random_seed=args.random_seed)
    
    # Test LM Studio connection
    if not writer.test_lm_studio_connection():
        print("\nPlease ensure LM Studio is running with a model loaded.")
        sys.exit(1)
    
    # Create team and run session
    writer.create_blog_writing_team(complexity=args.complexity)
    results = writer.run_blog_writing_session(
        topic=args.topic,
        simulation_time=args.simulation_time
    )
    
    # Display and optionally save results
    writer.display_results(results)
    
    if args.save_output:
        writer.save_results(results, args.save_output)


if __name__ == "__main__":
    main()